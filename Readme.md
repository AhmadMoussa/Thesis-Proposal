# Master Thesis

* [Master Kullback Leibler Divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)


Kullback leibler divergence explained
https://medium.com/@samsachedina/demystified-kullback-leibler-divergence-3971f956ef34


Video explaining information entropy
https://www.youtube.com/watch?v=LodZWzrbayY


Course on information entropy MIT
https://www.youtube.com/watch?list=PLDDE03B3BDCA1D9B1&v=phxsQrZQupo


Introduction to the Adam Optimizer
https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/


Introduction to Autoencoders
https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f


Tensorflow code for different autoencoders
https://github.com/nathanhubens/Autoencoders


Understanding and Optimizing Gans
https://towardsdatascience.com/understanding-and-optimizing-gans-going-back-to-first-principles-e5df8835ae18


This guy has a very Interesting blog
https://www.countbayesie.com/all-posts

Optimization problem solved by crowdsourcing, Maybe I can make somthing similar to this
https://koyama.xyz/project/sequential_line_search/

Why use the log probability for gradient descent instead of just the probability, very interestion:
https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability

Overview of convolutional network part1:
https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/
part 2:
https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/

Course of convolutional neural networks:
http://cs231n.github.io/convolutional-networks/
http://cs231n.github.io/

