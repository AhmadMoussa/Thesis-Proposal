# Master Thesis

## Probability Concepts for machine learning:
* [Kullback Leibler Divergence Explained in Detail, Count Bayesie](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)
* [Kullback leibler divergence overview, Medium Article](https://medium.com/@samsachedina/demystified-kullback-leibler-divergence-3971f956ef34)
* [Video explaining information entropy](https://www.youtube.com/watch?v=LodZWzrbayY)
* [Course on information entropy MIT](https://www.youtube.com/watch?list=PLDDE03B3BDCA1D9B1&v=phxsQrZQupo)

## Auto-encoders:
* [Introduction to Autoencoders](https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f)
* [Tensorflow code for different autoencoders](https://github.com/nathanhubens/Autoencoders)

## GANs:
* [Understanding and Optimizing Gans](https://towardsdatascience.com/understanding-and-optimizing-gans-going-back-to-first-principles-e5df8835ae18)

## Other relevant material:
* [This guy has a very Interesting blog](https://www.countbayesie.com/all-posts)
* [Why use the log probability for gradient descent instead of just the probability, very interestion!](https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability)

## Optimization techniques:
* [Optimization problem solved by crowdsourcing, Maybe I can make somthing similar to this](https://koyama.xyz/project/sequential_line_search/)
* [Introduction to the Adam Optimizer](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)

## Convolutional networks:
* [Understanding Convolutions](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/), [part2](
https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)
* [Stanford course on convolutional networks](http://cs231n.github.io/)



